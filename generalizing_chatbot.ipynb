{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22084a51",
   "metadata": {},
   "source": [
    "# My LLM Client Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244e5da1",
   "metadata": {},
   "source": [
    "On my journey of developing smart chatbots, i ran into rate limits while experimenting with open ai. This was after I had spent hours setting up my system. I knew that was not the end of the world. I then decided to implement a generalized way to program LLMs. That way I could retain cloud llms with different price points while incorporating local models like Ollama. I named my chatbot TUTI which translates to parrot. So much fun! So, here we go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b17e516",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In this notebook, I implement a multi-provider LLM client with error handling and retry logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e096bbb",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Firstly, I install dependencies and set up API keys for the different LLM providers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ea449c",
   "metadata": {},
   "source": [
    "### install dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75374c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this cell once, then comment it out)\n",
    "# %pip install openai\n",
    "\n",
    "# %pip install openai # installs the OpenAI API client.\n",
    "# %pip install python-dotenv # installs the dotenv package for loading environment variables.\n",
    "\n",
    "# %pip install openai google-generativeai anthropic tenacity python-dotenv\n",
    "\n",
    "# %pip install ollama  # installs the Ollama API client.\n",
    "\n",
    "# Download an Ollama model (run this cell once, then comment it out)\n",
    "# !ollama pull llama3:8b  # Good balance of size and quality\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23240a98",
   "metadata": {},
   "source": [
    "### import libraries and set up api keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6d7903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything needed\n",
    "import os\n",
    "from enum import Enum\n",
    "from typing import List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI as OpenAIClient\n",
    "import google.generativeai as genai\n",
    "import anthropic\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "\n",
    "import time\n",
    "\n",
    "import ollama\n",
    "# import json\n",
    "\n",
    "\n",
    "# Set API keys (i load from .env file)\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['DEEPSEEK_API_KEY'] = os.getenv('DEEPSEEK_API_KEY')  \n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "# ... etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fce7f11",
   "metadata": {},
   "source": [
    "### data classes and enums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b9591aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining data structures i will use\n",
    "class LLMProvider(Enum):\n",
    "    OPENAI = \"openai\"\n",
    "    DEEPSEEK = \"deepseek\"\n",
    "    GOOGLE = \"google\"\n",
    "    ANTHROPIC = \"anthropic\"\n",
    "    OLLAMA = \"ollama\" # NEW: Added Ollama as a provider for local models\n",
    "\n",
    "@dataclass\n",
    "class LLMResponse:\n",
    "    content: str\n",
    "    provider: LLMProvider\n",
    "    model: str\n",
    "    usage: Optional[Dict] = None\n",
    "    latency: Optional[float] = None\n",
    "    local: bool = False  # NEW: Flag for local vs cloud because now i have ollama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b8c5d9",
   "metadata": {},
   "source": [
    "### main LLM client class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "edf617c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProfessionalLLMClient:\n",
    "    \"\"\"Main client class to interact with various LLM providers.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.clients = {}\n",
    "        self.available_providers = []\n",
    "        self._initialize_all_clients()\n",
    "\n",
    "    \n",
    "    def _initialize_all_clients(self):\n",
    "        \"\"\"Initialize all available API clients.\"\"\"\n",
    "        provider_configs = {\n",
    "            # LLMProvider.OPENAI: {\n",
    "            #     'env_var': 'OPENAI_API_KEY',\n",
    "            #     'init_func': self._init_openai,\n",
    "            #     'requires_key': True  # NEW: Flag for key requirement\n",
    "            # },\n",
    "            # LLMProvider.DEEPSEEK: {\n",
    "            #     'env_var': 'DEEPSEEK_API_KEY', \n",
    "            #     'init_func': self._init_deepseek,\n",
    "            #     'requires_key': True\n",
    "            # },\n",
    "            # LLMProvider.GOOGLE: {\n",
    "            #     'env_var': 'GOOGLE_API_KEY',\n",
    "            #     'init_func': self._init_google,\n",
    "            #     'requires_key': True\n",
    "            # },\n",
    "            # LLMProvider.ANTHROPIC: {\n",
    "            #     'env_var': 'ANTHROPIC_API_KEY',\n",
    "            #     'init_func': self._init_anthropic,\n",
    "            #     'requires_key': True\n",
    "            # },\n",
    "            LLMProvider.OLLAMA: {\n",
    "                'env_var': None,  # No environment variable needed\n",
    "                'init_func': self._init_ollama,\n",
    "                'requires_key': False  # NEW: No API key required!\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for provider, config in provider_configs.items():\n",
    "            try:\n",
    "                if config['requires_key']:\n",
    "                    # For providers that need API keys\n",
    "                    api_key = os.getenv(config['env_var'])\n",
    "                    if api_key and not api_key.startswith('your-'):\n",
    "                        config['init_func'](api_key)\n",
    "                        self.available_providers.append(provider)\n",
    "                        print(f\"âœ… {provider.value.upper()} client initialized successfully\")\n",
    "                    else:\n",
    "                        print(f\"âš ï¸  Skipping {provider.value}: API key not found\")\n",
    "                else:\n",
    "                    # For providers that don't need API keys (like Ollama)\n",
    "                    config['init_func'](None)  # Pass None or whatever your init expects\n",
    "                    self.available_providers.append(provider)\n",
    "                    print(f\"âœ… {provider.value.upper()} client initialized successfully\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Failed to initialize {provider.value}: {e}\")\n",
    "    \n",
    "\n",
    "    def _init_ollama(self, api_key=None):\n",
    "        \"\"\"Initialize Ollama client - no API key needed!\"\"\"\n",
    "        try:\n",
    "            # Test if Ollama is running\n",
    "            ollama.list()\n",
    "            self.clients[LLMProvider.OLLAMA] = ollama\n",
    "            print(\"Ollama is running and ready!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Ollama not available: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _init_openai(self, api_key: str):\n",
    "        \"\"\"Initialize OpenAI client.\"\"\"\n",
    "        self.clients[LLMProvider.OPENAI] = OpenAIClient(api_key=api_key)\n",
    "    \n",
    "    def _init_deepseek(self, api_key: str):\n",
    "        \"\"\"Initialize DeepSeek client.\"\"\"\n",
    "        self.clients[LLMProvider.DEEPSEEK] = OpenAIClient(\n",
    "            api_key=api_key,\n",
    "            base_url=\"https://api.deepseek.com/v1\"\n",
    "        )\n",
    "    \n",
    "    def _init_google(self, api_key: str):\n",
    "        \"\"\"Initialize Google client.\"\"\"\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.clients[LLMProvider.GOOGLE] = genai\n",
    "    \n",
    "    def _init_anthropic(self, api_key: str):\n",
    "        \"\"\"Initialize Anthropic client.\"\"\"\n",
    "        self.clients[LLMProvider.ANTHROPIC] = anthropic.Anthropic(api_key=api_key)\n",
    "    \n",
    "    @retry(\n",
    "        retry=retry_if_exception_type((\n",
    "            openai.RateLimitError,\n",
    "            anthropic.RateLimitError,\n",
    "            openai.APIConnectionError,\n",
    "            openai.APIError\n",
    "        )),\n",
    "        wait=wait_exponential(multiplier=1, min=2, max=30),\n",
    "        stop=stop_after_attempt(3),\n",
    "        reraise=True\n",
    "    )\n",
    "    def chat_completion(\n",
    "        self,\n",
    "        provider: LLMProvider,\n",
    "        messages: List[Dict[str, str]],\n",
    "        model: Optional[str] = None,\n",
    "        temperature: float = 0.7,\n",
    "        max_tokens: Optional[int] = None,\n",
    "        **kwargs\n",
    "    ) -> LLMResponse:\n",
    "        \"\"\"\n",
    "        Chat completion with comprehensive error handling.\n",
    "        \n",
    "        Args:\n",
    "            provider: The LLM provider to use\n",
    "            messages: List of message dictionaries\n",
    "            model: Specific model name (uses default if None)\n",
    "            temperature: Creativity control (0.0 to 1.0)\n",
    "            max_tokens: Maximum tokens to generate\n",
    "            \n",
    "        Returns:\n",
    "            LLMResponse object with structured data\n",
    "        \"\"\"\n",
    "        \n",
    "        if provider not in self.clients:\n",
    "            raise ValueError(f\"Provider {provider.value} not available\")\n",
    "        \n",
    "        # Set default models\n",
    "        default_models = {\n",
    "            LLMProvider.OPENAI: \"gpt-3.5-turbo\",\n",
    "            LLMProvider.DEEPSEEK: \"deepseek-chat\",\n",
    "            LLMProvider.GOOGLE: \"gemini-1.0-pro\",  # Updated from gemini-pro\n",
    "            LLMProvider.ANTHROPIC: \"claude-3-haiku-20240307\",\n",
    "            LLMProvider.OLLAMA: \"llama3:8b\"  # NEW: Added default model for Ollama\n",
    "        }\n",
    "        model = model or default_models[provider]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            if provider == LLMProvider.OLLAMA:\n",
    "                # Convert messages to Ollama format\n",
    "                ollama_messages = []\n",
    "                for msg in messages:\n",
    "                    ollama_messages.append({\n",
    "                        'role': msg['role'],\n",
    "                        'content': msg['content']\n",
    "                    })\n",
    "                \n",
    "                # Make Ollama API call\n",
    "                response = self.clients[provider].chat(\n",
    "                    model=model,\n",
    "                    messages=ollama_messages,\n",
    "                    options={\n",
    "                        'temperature': temperature,\n",
    "                        'num_predict': max_tokens or 512,  # Ollama uses num_predict\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                content = response['message']['content']\n",
    "                usage = {\n",
    "                    'prompt_tokens': response.get('prompt_eval_count', 0),\n",
    "                    'completion_tokens': response.get('eval_count', 0),\n",
    "                    'total_tokens': response.get('prompt_eval_count', 0) + response.get('eval_count', 0)\n",
    "                }\n",
    "            elif provider in [LLMProvider.OPENAI, LLMProvider.DEEPSEEK]:\n",
    "                response = self.clients[provider].chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=max_tokens,\n",
    "                    **kwargs\n",
    "                )\n",
    "                content = response.choices[0].message.content\n",
    "                usage = {\n",
    "                    'prompt_tokens': response.usage.prompt_tokens,\n",
    "                    'completion_tokens': response.usage.completion_tokens,\n",
    "                    'total_tokens': response.usage.total_tokens\n",
    "                } if hasattr(response, 'usage') and response.usage else None\n",
    "                \n",
    "            elif provider == LLMProvider.GOOGLE:\n",
    "                # UPDATED: Google Gemini API format\n",
    "                conversation = []\n",
    "                for msg in messages:\n",
    "                    if msg[\"role\"] == \"system\":\n",
    "                        # For system messages, prepend to first user message\n",
    "                        if conversation and conversation[-1][\"role\"] == \"user\":\n",
    "                            conversation[-1][\"parts\"][0] = f\"{msg['content']}\\n\\n{conversation[-1]['parts'][0]}\"\n",
    "                        else:\n",
    "                            # If no user message yet, store system for later\n",
    "                            system_content = msg[\"content\"]\n",
    "                    else:\n",
    "                        role = \"user\" if msg[\"role\"] == \"user\" else \"model\"\n",
    "                        conversation.append({\"role\": role, \"parts\": [msg[\"content\"]]})\n",
    "                \n",
    "                # Create the model\n",
    "                model_obj = self.clients[provider].GenerativeModel(\n",
    "                    model_name=model,\n",
    "                    generation_config={\n",
    "                        \"temperature\": temperature,\n",
    "                        \"max_output_tokens\": max_tokens or 2048,\n",
    "                        \"top_p\": kwargs.get(\"top_p\", 0.95),\n",
    "                        \"top_k\": kwargs.get(\"top_k\", 40),\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Start chat and send message\n",
    "                chat = model_obj.start_chat(history=conversation[:-1] if len(conversation) > 1 else [])\n",
    "                response = chat.send_message(conversation[-1][\"parts\"][0] if conversation else \"\")\n",
    "                \n",
    "                content = response.text\n",
    "                usage = {\n",
    "                    'prompt_tokens': response.usage_metadata.prompt_token_count if hasattr(response, 'usage_metadata') else None,\n",
    "                    'completion_tokens': response.usage_metadata.candidates_token_count if hasattr(response, 'usage_metadata') else None,\n",
    "                    'total_tokens': response.usage_metadata.total_token_count if hasattr(response, 'usage_metadata') else None\n",
    "                }\n",
    "\n",
    "                \n",
    "            elif provider == LLMProvider.ANTHROPIC:\n",
    "                # Convert to Anthropic's format\n",
    "                system_content = None\n",
    "                anthropic_messages = []\n",
    "                \n",
    "                for msg in messages:\n",
    "                    if msg[\"role\"] == \"system\":\n",
    "                        system_content = msg[\"content\"]\n",
    "                    else:\n",
    "                        role = \"user\" if msg[\"role\"] == \"user\" else \"assistant\"\n",
    "                        anthropic_messages.append({\"role\": role, \"content\": msg[\"content\"]})\n",
    "                \n",
    "                response = self.clients[provider].messages.create(\n",
    "                    model=model,\n",
    "                    messages=anthropic_messages,\n",
    "                    system=system_content,\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=max_tokens or 1024,\n",
    "                    **kwargs\n",
    "                )\n",
    "                content = response.content[0].text\n",
    "                usage = {\n",
    "                    'input_tokens': response.usage.input_tokens,\n",
    "                    'output_tokens': response.usage.output_tokens\n",
    "                }\n",
    "\n",
    "            \n",
    "                \n",
    "                \n",
    "            latency = time.time() - start_time\n",
    "            \n",
    "            return LLMResponse(\n",
    "                content=content,\n",
    "                provider=provider,\n",
    "                model=model,\n",
    "                usage=usage,\n",
    "                latency=latency\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {provider.value} API: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def get_available_providers(self) -> List[LLMProvider]:\n",
    "        \"\"\"Get list of successfully initialized providers.\"\"\"\n",
    "        return self.available_providers\n",
    "    \n",
    "    def get_provider_models(self, provider: LLMProvider) -> List[str]:\n",
    "        \"\"\"Get available models for a provider.\"\"\"\n",
    "        model_lists = {\n",
    "            LLMProvider.OPENAI: [\"gpt-4-turbo\", \"gpt-4\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-16k\"],\n",
    "            LLMProvider.DEEPSEEK: [\"deepseek-chat\", \"deepseek-coder\"],\n",
    "            LLMProvider.GOOGLE: [\n",
    "                \"gemini-1.0-pro\",      # General purpose\n",
    "                \"gemini-1.0-pro-001\",  # Specific version\n",
    "                \"gemini-pro\",           # Some accounts might still have this\n",
    "                \"gemini-1.5-pro-latest\" # Latest 1.5 pro (if you have access)\n",
    "            ],\n",
    "            LLMProvider.ANTHROPIC: [\n",
    "                \"claude-3-opus-20240229\", \n",
    "                \"claude-3-sonnet-20240229\", \n",
    "                \"claude-3-haiku-20240307\"\n",
    "            ],\n",
    "            LLMProvider.OLLAMA: self._get_ollama_models()  # Dynamic list!\n",
    "            \n",
    "        }\n",
    "        return model_lists.get(provider, [])\n",
    "    \n",
    "    def _get_ollama_models(self) -> List[str]:\n",
    "        \"\"\"Get list of available Ollama models.\"\"\"\n",
    "        try:\n",
    "            result = self.clients[LLMProvider.OLLAMA].list()\n",
    "            return [model['name'] for model in result['models']]\n",
    "        except:\n",
    "            return [\"llama3:8b\", \"llama3:70b\", \"mixtral:8x7b\", \"gemma:7b\", \"phi3:mini\"]\n",
    "\n",
    "    def batch_process(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        provider: LLMProvider,\n",
    "        **kwargs\n",
    "    ) -> List[LLMResponse]:\n",
    "        \"\"\"Process multiple prompts with the same settings.\"\"\"\n",
    "        results = []\n",
    "        for prompt in prompts:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            results.append(self.chat_completion(provider, messages, **kwargs))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d64d76",
   "metadata": {},
   "source": [
    "### initializing my LLM client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56777d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running and ready!\n",
      "âœ… OLLAMA client initialized successfully\n",
      "Available providers: ['ollama']\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of my llm client\n",
    "llm_client = ProfessionalLLMClient()\n",
    "\n",
    "# Check what providers are available\n",
    "print(\"Available providers:\", [p.value for p in llm_client.get_available_providers()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71c63af",
   "metadata": {},
   "source": [
    "## Usage Examples\n",
    "Here's how to use the client for different providers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535928a7",
   "metadata": {},
   "source": [
    "### testing my individual providers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a30b9f",
   "metadata": {},
   "source": [
    "#### deepseek / openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "57495be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Provider deepseek not available\n"
     ]
    }
   ],
   "source": [
    "# Testing my LLM providers, one at a time\n",
    "test_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain quantum computing to me like i am a farmer\"}\n",
    "]\n",
    "\n",
    "try:\n",
    "    response = llm_client.chat_completion(LLMProvider.DEEPSEEK, test_messages)\n",
    "    print(\"DeepSeek Response:\", response.content)\n",
    "    print(f\"Latency: {response.latency:.2f}s\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80a9208",
   "metadata": {},
   "source": [
    "#### google gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c2049c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Google models: ['gemini-1.0-pro', 'gemini-1.0-pro-001', 'gemini-pro', 'gemini-1.5-pro-latest']\n",
      "\n",
      "ğŸ§ª Testing Google model: gemini-1.0-pro\n",
      "âŒ gemini-1.0-pro failed: Provider google not available\n",
      "\n",
      "ğŸ§ª Testing Google model: gemini-1.0-pro-001\n",
      "âŒ gemini-1.0-pro-001 failed: Provider google not available\n",
      "\n",
      "ğŸ§ª Testing Google model: gemini-pro\n",
      "âŒ gemini-pro failed: Provider google not available\n",
      "\n",
      "ğŸ§ª Testing Google model: gemini-1.5-pro-latest\n",
      "âŒ gemini-1.5-pro-latest failed: Provider google not available\n"
     ]
    }
   ],
   "source": [
    "# Test Google Gemini specifically with different models\n",
    "test_messages = [{\"role\": \"user\", \"content\": \"Hello! What's the weather like today?\"}]\n",
    "\n",
    "# Try available Google models\n",
    "google_models = llm_client.get_provider_models(LLMProvider.GOOGLE)\n",
    "print(\"Available Google models:\", google_models)\n",
    "\n",
    "for model in google_models:\n",
    "    try:\n",
    "        print(f\"\\nğŸ§ª Testing Google model: {model}\")\n",
    "        response = llm_client.chat_completion(\n",
    "            LLMProvider.GOOGLE, \n",
    "            test_messages, \n",
    "            model=model\n",
    "        )\n",
    "        print(f\"âœ… Success with {model}: {response.content[:100]}...\")\n",
    "        break  # Stop after first successful model\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {model} failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41219ddb",
   "metadata": {},
   "source": [
    "#### ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "2243461d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§ª Testing Ollama...\n",
      "âœ… Ollama response: Quantum computing! It's a fascinating topic that c...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nğŸ§ª Testing Ollama...\")\n",
    "try:\n",
    "    test_messages = [{\"role\": \"user\", \"content\": \"Explain quantum computing simply\"}]\n",
    "    response = llm_client.chat_completion(LLMProvider.OLLAMA, test_messages)\n",
    "    print(f\"âœ… Ollama response: {response.content[:50]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Ollama test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c87e5b6",
   "metadata": {},
   "source": [
    "## Comparison Function\n",
    "This function compares the performance of different providers, comparing parameters like latency, token usage, and the quality of response content. Please try it for yourself :) In an effort not to bias your own analyses, I will reserve my findings to myself :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "12b8e07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Testing ollama...\n",
      "âœ… ollama completed in 144.37s\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Compare multiple providers\n",
    "def compare_providers_professional(prompt: str):\n",
    "    \"\"\"Professional comparison of all available providers.\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    results = {}\n",
    "    \n",
    "    for provider in llm_client.get_available_providers():\n",
    "        try:\n",
    "            print(f\"ğŸ§ª Testing {provider.value}...\")\n",
    "            response = llm_client.chat_completion(provider, messages, temperature=0.7)\n",
    "            results[provider.value] = {\n",
    "                'content': response.content,\n",
    "                'latency': response.latency,\n",
    "                'tokens': response.usage\n",
    "            }\n",
    "            print(f\"âœ… {provider.value} completed in {response.latency:.2f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[provider.value] = f\"Error: {str(e)}\"\n",
    "            print(f\"âŒ {provider.value} failed: {e}\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run comparison\n",
    "comparison_results = compare_providers_professional(\n",
    "    \"What are the main benefits of machine learning in healthcare?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01a673f",
   "metadata": {},
   "source": [
    "### Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bc09f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ OLLAMA:\n",
      "â±ï¸  Latency: 144.37s\n",
      "ğŸ“Š Tokens: {'prompt_tokens': 21, 'completion_tokens': 467, 'total_tokens': 488}\n",
      "ğŸ’¬ Response: Machine learning (ML) is revolutionizing healthcare by transforming clinical decision-making, improving patient outcomes, and reducing costs. The primary benefits of ML in healthcare include:\n",
      "\n",
      "1. **Pe...\n",
      "â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯\n"
     ]
    }
   ],
   "source": [
    "# Display results in a nice format\n",
    "for provider, data in comparison_results.items():\n",
    "    if isinstance(data, dict):\n",
    "        print(f\"\\nğŸ¯ {provider.upper()}:\")\n",
    "        print(f\"â±ï¸ Latency: {data['latency']:.2f}s\")\n",
    "        print(f\"ğŸ“Š Tokens: {data['tokens']}\")\n",
    "        print(f\"ğŸ’¬ Response: {data['content'][:200]}...\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ {provider.upper()}: {data}\")\n",
    "    print(\"â¯\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731c1fe1",
   "metadata": {},
   "source": [
    "## Experimenting with Batch Processing\n",
    "This enables us to ask the LLM provider multiple prompts and print out the model's performance results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c9bd3b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch of prompts with DeepSeek...\n",
      "\n",
      "ğŸ“ Prompt 1: Explain blockchain technology...\n",
      "ğŸ’¡ Response: The fascinating world of blockchain technology!\n",
      "\n",
      "Blockchain is a decentralized, digital ledger techn...\n",
      "â±ï¸  Latency: 145.99s\n",
      "\n",
      "ğŸ“ Prompt 2: What is the capital of France?...\n",
      "ğŸ’¡ Response: The capital of France is Paris....\n",
      "â±ï¸  Latency: 3.56s\n",
      "\n",
      "ğŸ“ Prompt 3: How does photosynthesis work?...\n",
      "ğŸ’¡ Response: Photosynthesis is the process by which plants, algae, and some bacteria convert light energy from th...\n",
      "â±ï¸  Latency: 157.48s\n"
     ]
    }
   ],
   "source": [
    "# Process multiple prompts at once\n",
    "prompts = [\n",
    "    \"Explain blockchain technology like I'm seven\",\n",
    "    \"What is the capital of Uganda?\",\n",
    "    \"How does pollination work?\"\n",
    "]\n",
    "\n",
    "print(\"Processing batch of prompts with Ollama...\")\n",
    "batch_results = llm_client.batch_process(\n",
    "    prompts=prompts,\n",
    "    provider=LLMProvider.OLLAMA,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "for i, result in enumerate(batch_results):\n",
    "    print(f\"\\nğŸ“ Prompt {i+1}: {prompts[i][:30]}...\")\n",
    "    print(f\"ğŸ’¡ Response: {result.content[:100]}...\")\n",
    "    print(f\"â±ï¸  Latency: {result.latency:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea32a0ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
