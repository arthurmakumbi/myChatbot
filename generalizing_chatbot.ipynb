{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22084a51",
   "metadata": {},
   "source": [
    "# My LLM Client Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244e5da1",
   "metadata": {},
   "source": [
    "On my journey of developing smart chatbots, i ran into rate limits while experimenting with open ai. This was after I had spent hours setting up my system. I knew that was not the end of the world. I then decided to implement a generalized way to program LLMs. That way I could retain cloud llms with different price points while incorporating local models like Ollama. I named my chatbot TUTI which translates to parrot. So much fun! So, here we go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b17e516",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In this notebook, I implement a multi-provider LLM client with error handling and retry logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e096bbb",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Firstly, I install dependencies and set up API keys for the different LLM providers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ea449c",
   "metadata": {},
   "source": [
    "### install dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75374c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this cell once, then comment it out)\n",
    "# %pip install openai\n",
    "\n",
    "# %pip install openai # installs the OpenAI API client.\n",
    "# %pip install python-dotenv # installs the dotenv package for loading environment variables.\n",
    "\n",
    "# %pip install openai google-generativeai anthropic tenacity python-dotenv\n",
    "\n",
    "# %pip install ollama  # installs the Ollama API client.\n",
    "\n",
    "# Download an Ollama model (run this cell once, then comment it out)\n",
    "# !ollama pull llama3:8b  # Good balance of size and quality\n",
    "\n",
    "# %pip install panel # for GUI\n",
    "# %pip install jupyter_bokeh\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23240a98",
   "metadata": {},
   "source": [
    "### import libraries and set up api keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e6d7903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything needed\n",
    "import os\n",
    "from enum import Enum\n",
    "from typing import List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI as OpenAIClient\n",
    "import google.generativeai as genai\n",
    "import anthropic\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "\n",
    "import time\n",
    "\n",
    "import panel as pn  # GUI\n",
    "\n",
    "import ollama\n",
    "# import json\n",
    "\n",
    "\n",
    "# Set API keys (i load from .env file)\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['DEEPSEEK_API_KEY'] = os.getenv('DEEPSEEK_API_KEY')  \n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "# ... etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fce7f11",
   "metadata": {},
   "source": [
    "### data classes and enums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9591aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining data structures i will use\n",
    "class LLMProvider(Enum):\n",
    "    OPENAI = \"openai\"\n",
    "    DEEPSEEK = \"deepseek\"\n",
    "    GOOGLE = \"google\"\n",
    "    ANTHROPIC = \"anthropic\"\n",
    "    OLLAMA = \"ollama\" #Added Ollama as a provider for local models\n",
    "\n",
    "@dataclass\n",
    "class LLMResponse:\n",
    "    content: str\n",
    "    provider: LLMProvider\n",
    "    model: str\n",
    "    usage: Optional[Dict] = None\n",
    "    latency: Optional[float] = None\n",
    "    local: bool = False  #Flag for local vs cloud because now i have ollama\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b8c5d9",
   "metadata": {},
   "source": [
    "### main LLM client class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edf617c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProfessionalLLMClient:\n",
    "    \"\"\"Main client class to interact with various LLM providers.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.clients = {}\n",
    "        self.available_providers = []\n",
    "        self._initialize_all_clients()\n",
    "\n",
    "    \n",
    "    def _initialize_all_clients(self):\n",
    "        \"\"\"Initialize all available API clients.\"\"\"\n",
    "        provider_configs = {\n",
    "            # LLMProvider.OPENAI: {\n",
    "            #     'env_var': 'OPENAI_API_KEY',\n",
    "            #     'init_func': self._init_openai,\n",
    "            #     'requires_key': True  #Flag for key requirement\n",
    "            # },\n",
    "            # LLMProvider.DEEPSEEK: {\n",
    "            #     'env_var': 'DEEPSEEK_API_KEY', \n",
    "            #     'init_func': self._init_deepseek,\n",
    "            #     'requires_key': True\n",
    "            # },\n",
    "            # LLMProvider.GOOGLE: {\n",
    "            #     'env_var': 'GOOGLE_API_KEY',\n",
    "            #     'init_func': self._init_google,\n",
    "            #     'requires_key': True\n",
    "            # },\n",
    "            # LLMProvider.ANTHROPIC: {\n",
    "            #     'env_var': 'ANTHROPIC_API_KEY',\n",
    "            #     'init_func': self._init_anthropic,\n",
    "            #     'requires_key': True\n",
    "            # },\n",
    "            LLMProvider.OLLAMA: {\n",
    "                'env_var': None,  # No environment variable needed\n",
    "                'init_func': self._init_ollama,\n",
    "                'requires_key': False  #No API key required!\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for provider, config in provider_configs.items():\n",
    "            try:\n",
    "                if config['requires_key']:\n",
    "                    # For providers that need API keys\n",
    "                    api_key = os.getenv(config['env_var'])\n",
    "                    if api_key and not api_key.startswith('your-'):\n",
    "                        config['init_func'](api_key)\n",
    "                        self.available_providers.append(provider)\n",
    "                        print(f\"‚úÖ {provider.value.upper()} client initialized successfully\")\n",
    "                    else:\n",
    "                        print(f\"‚ö†Ô∏è  Skipping {provider.value}: API key not found\")\n",
    "                else:\n",
    "                    # For providers that don't need API keys (like Ollama)\n",
    "                    config['init_func'](None)  # Pass None or whatever the init expects\n",
    "                    self.available_providers.append(provider)\n",
    "                    print(f\"‚úÖ {provider.value.upper()} client initialized successfully\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to initialize {provider.value}: {e}\")\n",
    "    \n",
    "\n",
    "    def _init_ollama(self, api_key=None):\n",
    "        \"\"\"Initialize Ollama client - no API key needed!\"\"\"\n",
    "        try:\n",
    "            # Test if Ollama is running\n",
    "            ollama.list()\n",
    "            self.clients[LLMProvider.OLLAMA] = ollama\n",
    "            print(\"Ollama is running and ready!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Ollama not available: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _init_openai(self, api_key: str):\n",
    "        \"\"\"Initialize OpenAI client.\"\"\"\n",
    "        self.clients[LLMProvider.OPENAI] = OpenAIClient(api_key=api_key)\n",
    "    \n",
    "    def _init_deepseek(self, api_key: str):\n",
    "        \"\"\"Initialize DeepSeek client.\"\"\"\n",
    "        self.clients[LLMProvider.DEEPSEEK] = OpenAIClient(\n",
    "            api_key=api_key,\n",
    "            base_url=\"https://api.deepseek.com/v1\"\n",
    "        )\n",
    "    \n",
    "    def _init_google(self, api_key: str):\n",
    "        \"\"\"Initialize Google client.\"\"\"\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.clients[LLMProvider.GOOGLE] = genai\n",
    "    \n",
    "    def _init_anthropic(self, api_key: str):\n",
    "        \"\"\"Initialize Anthropic client.\"\"\"\n",
    "        self.clients[LLMProvider.ANTHROPIC] = anthropic.Anthropic(api_key=api_key)\n",
    "    \n",
    "    @retry(\n",
    "        retry=retry_if_exception_type((\n",
    "            openai.RateLimitError,\n",
    "            anthropic.RateLimitError,\n",
    "            openai.APIConnectionError,\n",
    "            openai.APIError\n",
    "        )),\n",
    "        wait=wait_exponential(multiplier=1, min=2, max=30),\n",
    "        stop=stop_after_attempt(3),\n",
    "        reraise=True\n",
    "    )\n",
    "    def chat_completion(\n",
    "        self,\n",
    "        provider: LLMProvider,\n",
    "        messages: List[Dict[str, str]],\n",
    "        model: Optional[str] = None,\n",
    "        temperature: float = 0.7,\n",
    "        max_tokens: Optional[int] = None,\n",
    "        **kwargs\n",
    "    ) -> LLMResponse:\n",
    "        \n",
    "        \"\"\"\n",
    "        Chat completion with comprehensive error handling.\n",
    "        \n",
    "        Args:\n",
    "            provider: The LLM provider to use\n",
    "            messages: List of message dictionaries\n",
    "            model: Specific model name (uses default if None)\n",
    "            temperature: Creativity control (0.0 to 1.0)\n",
    "            max_tokens: Maximum tokens to generate\n",
    "            \n",
    "        Returns:\n",
    "            LLMResponse object with structured data\n",
    "        \"\"\"\n",
    "        \n",
    "        if provider not in self.clients:\n",
    "            raise ValueError(f\"Provider {provider.value} not available\")\n",
    "        \n",
    "        # Set default models\n",
    "        default_models = {\n",
    "            LLMProvider.OPENAI: \"gpt-3.5-turbo\",\n",
    "            LLMProvider.DEEPSEEK: \"deepseek-chat\",\n",
    "            LLMProvider.GOOGLE: \"gemini-1.0-pro\",  # Updated from gemini-pro\n",
    "            LLMProvider.ANTHROPIC: \"claude-3-haiku-20240307\",\n",
    "            LLMProvider.OLLAMA: \"llama3:8b\"  #Added default model for Ollama\n",
    "        }\n",
    "        model = model or default_models[provider]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            if provider == LLMProvider.OLLAMA:\n",
    "                # Convert messages to Ollama format\n",
    "                ollama_messages = []\n",
    "                for msg in messages:\n",
    "                    ollama_messages.append({\n",
    "                        'role': msg['role'],\n",
    "                        'content': msg['content']\n",
    "                    })\n",
    "                \n",
    "                # Make Ollama API call\n",
    "                response = self.clients[provider].chat(\n",
    "                    model=model,\n",
    "                    messages=ollama_messages,\n",
    "                    options={\n",
    "                        'temperature': temperature,\n",
    "                        'num_predict': max_tokens or 512,  # Ollama uses num_predict\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                content = response['message']['content']\n",
    "                usage = {\n",
    "                    'prompt_tokens': response.get('prompt_eval_count', 0),\n",
    "                    'completion_tokens': response.get('eval_count', 0),\n",
    "                    'total_tokens': response.get('prompt_eval_count', 0) + response.get('eval_count', 0)\n",
    "                }\n",
    "            elif provider in [LLMProvider.OPENAI, LLMProvider.DEEPSEEK]:\n",
    "                response = self.clients[provider].chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=max_tokens,\n",
    "                    **kwargs\n",
    "                )\n",
    "                content = response.choices[0].message.content\n",
    "                usage = {\n",
    "                    'prompt_tokens': response.usage.prompt_tokens,\n",
    "                    'completion_tokens': response.usage.completion_tokens,\n",
    "                    'total_tokens': response.usage.total_tokens\n",
    "                } if hasattr(response, 'usage') and response.usage else None\n",
    "                \n",
    "            elif provider == LLMProvider.GOOGLE:\n",
    "                # UPDATED: Google Gemini API format\n",
    "                conversation = []\n",
    "                for msg in messages:\n",
    "                    if msg[\"role\"] == \"system\":\n",
    "                        # For system messages, prepend to first user message\n",
    "                        if conversation and conversation[-1][\"role\"] == \"user\":\n",
    "                            conversation[-1][\"parts\"][0] = f\"{msg['content']}\\n\\n{conversation[-1]['parts'][0]}\"\n",
    "                        else:\n",
    "                            # If no user message yet, store system for later\n",
    "                            system_content = msg[\"content\"]\n",
    "                    else:\n",
    "                        role = \"user\" if msg[\"role\"] == \"user\" else \"model\"\n",
    "                        conversation.append({\"role\": role, \"parts\": [msg[\"content\"]]})\n",
    "                \n",
    "                # Create the model\n",
    "                model_obj = self.clients[provider].GenerativeModel(\n",
    "                    model_name=model,\n",
    "                    generation_config={\n",
    "                        \"temperature\": temperature,\n",
    "                        \"max_output_tokens\": max_tokens or 2048,\n",
    "                        \"top_p\": kwargs.get(\"top_p\", 0.95),\n",
    "                        \"top_k\": kwargs.get(\"top_k\", 40),\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Start chat and send message\n",
    "                chat = model_obj.start_chat(history=conversation[:-1] if len(conversation) > 1 else [])\n",
    "                response = chat.send_message(conversation[-1][\"parts\"][0] if conversation else \"\")\n",
    "                \n",
    "                content = response.text\n",
    "                usage = {\n",
    "                    'prompt_tokens': response.usage_metadata.prompt_token_count if hasattr(response, 'usage_metadata') else None,\n",
    "                    'completion_tokens': response.usage_metadata.candidates_token_count if hasattr(response, 'usage_metadata') else None,\n",
    "                    'total_tokens': response.usage_metadata.total_token_count if hasattr(response, 'usage_metadata') else None\n",
    "                }\n",
    "\n",
    "                \n",
    "            elif provider == LLMProvider.ANTHROPIC:\n",
    "                # Convert to Anthropic's format\n",
    "                system_content = None\n",
    "                anthropic_messages = []\n",
    "                \n",
    "                for msg in messages:\n",
    "                    if msg[\"role\"] == \"system\":\n",
    "                        system_content = msg[\"content\"]\n",
    "                    else:\n",
    "                        role = \"user\" if msg[\"role\"] == \"user\" else \"assistant\"\n",
    "                        anthropic_messages.append({\"role\": role, \"content\": msg[\"content\"]})\n",
    "                \n",
    "                response = self.clients[provider].messages.create(\n",
    "                    model=model,\n",
    "                    messages=anthropic_messages,\n",
    "                    system=system_content,\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=max_tokens or 1024,\n",
    "                    **kwargs\n",
    "                )\n",
    "                content = response.content[0].text\n",
    "                usage = {\n",
    "                    'input_tokens': response.usage.input_tokens,\n",
    "                    'output_tokens': response.usage.output_tokens\n",
    "                }\n",
    "\n",
    "            \n",
    "                \n",
    "                \n",
    "            latency = time.time() - start_time\n",
    "            \n",
    "            return LLMResponse(\n",
    "                content=content,\n",
    "                provider=provider,\n",
    "                model=model,\n",
    "                usage=usage,\n",
    "                latency=latency\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {provider.value} API: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def get_available_providers(self) -> List[LLMProvider]:\n",
    "        \"\"\"Get list of successfully initialized providers.\"\"\"\n",
    "        return self.available_providers\n",
    "    \n",
    "    def get_provider_models(self, provider: LLMProvider) -> List[str]:\n",
    "        \"\"\"Get available models for a provider.\"\"\"\n",
    "        model_lists = {\n",
    "            LLMProvider.OPENAI: [\"gpt-4-turbo\", \"gpt-4\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-16k\"],\n",
    "            LLMProvider.DEEPSEEK: [\"deepseek-chat\", \"deepseek-coder\"],\n",
    "            LLMProvider.GOOGLE: [\n",
    "                \"gemini-1.0-pro\",      # General purpose\n",
    "                \"gemini-1.0-pro-001\",  # Specific version\n",
    "                \"gemini-pro\",           # Some accounts might still have this\n",
    "                \"gemini-1.5-pro-latest\" # Latest 1.5 pro (if you have access)\n",
    "            ],\n",
    "            LLMProvider.ANTHROPIC: [\n",
    "                \"claude-3-opus-20240229\", \n",
    "                \"claude-3-sonnet-20240229\", \n",
    "                \"claude-3-haiku-20240307\"\n",
    "            ],\n",
    "            LLMProvider.OLLAMA: self._get_ollama_models()  # Dynamic list!\n",
    "            \n",
    "        }\n",
    "        return model_lists.get(provider, [])\n",
    "    \n",
    "    def _get_ollama_models(self) -> List[str]:\n",
    "        \"\"\"Get list of available Ollama models.\"\"\"\n",
    "        try:\n",
    "            result = self.clients[LLMProvider.OLLAMA].list()\n",
    "            return [model['name'] for model in result['models']]\n",
    "        except:\n",
    "            return [\"llama3:8b\", \"llama3:70b\", \"mixtral:8x7b\", \"gemma:7b\", \"phi3:mini\"]\n",
    "\n",
    "    def batch_process(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        provider: LLMProvider,\n",
    "        **kwargs\n",
    "    ) -> List[LLMResponse]:\n",
    "        \"\"\"Process multiple prompts with the same settings.\"\"\"\n",
    "        results = []\n",
    "        for prompt in prompts:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            results.append(self.chat_completion(provider, messages, **kwargs))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d64d76",
   "metadata": {},
   "source": [
    "### initializing my LLM client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c56777d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is running and ready!\n",
      "‚úÖ OLLAMA client initialized successfully\n",
      "Available providers: ['ollama']\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of my llm client\n",
    "llm_client = ProfessionalLLMClient()\n",
    "\n",
    "# Check what providers are available\n",
    "print(\"Available providers:\", [p.value for p in llm_client.get_available_providers()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71c63af",
   "metadata": {},
   "source": [
    "## Usage Examples\n",
    "Here's how to use the client for different providers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535928a7",
   "metadata": {},
   "source": [
    "### testing my individual providers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a30b9f",
   "metadata": {},
   "source": [
    "#### deepseek / openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57495be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Provider deepseek not available\n"
     ]
    }
   ],
   "source": [
    "# Testing my LLM providers, one at a time\n",
    "test_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain quantum computing to me like i am a farmer\"}\n",
    "]\n",
    "\n",
    "try:\n",
    "    response = llm_client.chat_completion(LLMProvider.DEEPSEEK, test_messages)\n",
    "    print(\"DeepSeek Response:\", response.content)\n",
    "    print(f\"Latency: {response.latency:.2f}s\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80a9208",
   "metadata": {},
   "source": [
    "#### google gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2049c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Google models: ['gemini-1.0-pro', 'gemini-1.0-pro-001', 'gemini-pro', 'gemini-1.5-pro-latest']\n",
      "\n",
      "üß™ Testing Google model: gemini-1.0-pro\n",
      "‚ùå gemini-1.0-pro failed: Provider google not available\n",
      "\n",
      "üß™ Testing Google model: gemini-1.0-pro-001\n",
      "‚ùå gemini-1.0-pro-001 failed: Provider google not available\n",
      "\n",
      "üß™ Testing Google model: gemini-pro\n",
      "‚ùå gemini-pro failed: Provider google not available\n",
      "\n",
      "üß™ Testing Google model: gemini-1.5-pro-latest\n",
      "‚ùå gemini-1.5-pro-latest failed: Provider google not available\n"
     ]
    }
   ],
   "source": [
    "# Test Google Gemini specifically with different models\n",
    "test_messages = [{\"role\": \"user\", \"content\": \"Hello! What's the weather like today?\"}]\n",
    "\n",
    "# Try available Google models\n",
    "google_models = llm_client.get_provider_models(LLMProvider.GOOGLE)\n",
    "print(\"Available Google models:\", google_models)\n",
    "\n",
    "for model in google_models:\n",
    "    try:\n",
    "        print(f\"\\nüß™ Testing Google model: {model}\")\n",
    "        response = llm_client.chat_completion(\n",
    "            LLMProvider.GOOGLE, \n",
    "            test_messages, \n",
    "            model=model\n",
    "        )\n",
    "        print(f\"‚úÖ Success with {model}: {response.content[:100]}...\")\n",
    "        break  # Stop after first successful model\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå {model} failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41219ddb",
   "metadata": {},
   "source": [
    "#### ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2243461d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ Testing Ollama...\n",
      "‚úÖ Ollama response: The fascinating world of quantum computing!\n",
      "\n",
      "**Cla...\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüß™ Testing Ollama...\")\n",
    "try:\n",
    "    test_messages = [{\"role\": \"user\", \"content\": \"Explain quantum computing simply\"}]\n",
    "    response = llm_client.chat_completion(LLMProvider.OLLAMA, test_messages)\n",
    "    print(f\"‚úÖ Ollama response: {response.content[:50]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Ollama test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c87e5b6",
   "metadata": {},
   "source": [
    "## Comparison Function\n",
    "This function compares the performance of different providers, comparing parameters like latency, token usage, and the quality of response content. Please try it for yourself :) In an effort not to bias your own analyses, I will reserve my findings to myself :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12b8e07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing ollama...\n",
      "‚úÖ ollama completed in 162.89s\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Compare multiple providers\n",
    "def compare_providers_professional(prompt: str):\n",
    "    \"\"\"Professional comparison of all available providers.\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    results = {}\n",
    "    \n",
    "    for provider in llm_client.get_available_providers():\n",
    "        try:\n",
    "            print(f\"üß™ Testing {provider.value}...\")\n",
    "            response = llm_client.chat_completion(provider, messages, temperature=0.7)\n",
    "            results[provider.value] = {\n",
    "                'content': response.content,\n",
    "                'latency': response.latency,\n",
    "                'tokens': response.usage\n",
    "            }\n",
    "            print(f\"‚úÖ {provider.value} completed in {response.latency:.2f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[provider.value] = f\"Error: {str(e)}\"\n",
    "            print(f\"‚ùå {provider.value} failed: {e}\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run comparison\n",
    "comparison_results = compare_providers_professional(\n",
    "    \"What are the main benefits of machine learning in healthcare?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01a673f",
   "metadata": {},
   "source": [
    "### Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59bc09f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ OLLAMA:\n",
      "‚è±Ô∏è Latency: 162.89s\n",
      "üìä Tokens: {'prompt_tokens': 21, 'completion_tokens': 512, 'total_tokens': 533}\n",
      "üí¨ Response: Machine learning (ML) has numerous benefits in healthcare, including:\n",
      "\n",
      "1. **Improved disease diagnosis**: ML algorithms can analyze medical images, laboratory results, and electronic health records to...\n",
      "‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ‚éØ\n"
     ]
    }
   ],
   "source": [
    "# Display results in a nice format\n",
    "for provider, data in comparison_results.items():\n",
    "    if isinstance(data, dict):\n",
    "        print(f\"\\nüéØ {provider.upper()}:\")\n",
    "        print(f\"‚è±Ô∏è Latency: {data['latency']:.2f}s\")\n",
    "        print(f\"üìä Tokens: {data['tokens']}\")\n",
    "        print(f\"üí¨ Response: {data['content'][:200]}...\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå {provider.upper()}: {data}\")\n",
    "    print(\"‚éØ\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731c1fe1",
   "metadata": {},
   "source": [
    "## Experimenting with Batch Processing\n",
    "This enables us to ask the LLM provider multiple prompts and print out the model's performance results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c9bd3b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch of prompts with DeepSeek...\n",
      "\n",
      "üìù Prompt 1: Explain blockchain technology...\n",
      "üí° Response: The fascinating world of blockchain technology!\n",
      "\n",
      "Blockchain is a decentralized, digital ledger techn...\n",
      "‚è±Ô∏è  Latency: 145.99s\n",
      "\n",
      "üìù Prompt 2: What is the capital of France?...\n",
      "üí° Response: The capital of France is Paris....\n",
      "‚è±Ô∏è  Latency: 3.56s\n",
      "\n",
      "üìù Prompt 3: How does photosynthesis work?...\n",
      "üí° Response: Photosynthesis is the process by which plants, algae, and some bacteria convert light energy from th...\n",
      "‚è±Ô∏è  Latency: 157.48s\n"
     ]
    }
   ],
   "source": [
    "# Process multiple prompts at once\n",
    "prompts = [\n",
    "    \"Explain blockchain technology like I'm seven\",\n",
    "    \"What is the capital of Uganda?\",\n",
    "    \"How does pollination work?\"\n",
    "]\n",
    "\n",
    "print(\"Processing batch of prompts with Ollama...\")\n",
    "batch_results = llm_client.batch_process(\n",
    "    prompts=prompts,\n",
    "    provider=LLMProvider.OLLAMA,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "for i, result in enumerate(batch_results):\n",
    "    print(f\"\\nüìù Prompt {i+1}: {prompts[i][:30]}...\")\n",
    "    print(f\"üí° Response: {result.content[:100]}...\")\n",
    "    print(f\"‚è±Ô∏è  Latency: {result.latency:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea32a0ef",
   "metadata": {},
   "source": [
    "# OrderBot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb710cc",
   "metadata": {},
   "source": [
    "In this OrderBot, we automate the collection of user prompts and assistant responses. This OrderBot will take orders at a pizza restaurant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fb5ca8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "98fb4e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_messages(_):\n",
    "    prompt = inp.value_input\n",
    "    inp.value = ''\n",
    "    context.append({'role':'user', 'content':f\"{prompt}\"})\n",
    "    response = llm_client.chat_completion(provider=LLMProvider.OLLAMA, messages=context) \n",
    "    context.append({'role':'assistant', 'content':f\"{response.content}\"})\n",
    "    panels.append(\n",
    "        pn.Row('User:', pn.pane.Markdown(prompt, width=600)))\n",
    "    panels.append(\n",
    "        pn.Row('Assistant:', pn.pane.Markdown(response.content, width=600)))\n",
    " \n",
    "    return pn.Column(*panels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65815b2",
   "metadata": {},
   "source": [
    "We go ahead and set up the user interface to collect user prompts and assistant responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45207f1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"esms-options\">{\"shimMode\": true}</script><style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "  > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n  const py_version = '3.8.0'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  const reloading = false;\n  const Bokeh = root.Bokeh;\n\n  // Set a timeout for this load but only if we are not already initializing\n  if (typeof (root._bokeh_timeout) === \"undefined\" || (force || !root._bokeh_is_initializing)) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      // Don't load bokeh if it is still initializing\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    } else if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      // There is nothing to load\n      run_callbacks();\n      return null;\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error(e) {\n      const src_el = e.srcElement\n      console.error(\"failed to load \" + (src_el.href || src_el.src));\n    }\n\n    const skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {}, 'shim': {}});\n      root._bokeh_is_loading = css_urls.length + 0;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    const existing_stylesheets = []\n    const links = document.getElementsByTagName('link')\n    for (let i = 0; i < links.length; i++) {\n      const link = links[i]\n      if (link.href != null) {\n        existing_stylesheets.push(link.href)\n      }\n    }\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const escaped = encodeURI(url)\n      if (existing_stylesheets.indexOf(escaped) !== -1) {\n        on_load()\n        continue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    var existing_scripts = []\n    const scripts = document.getElementsByTagName('script')\n    for (let i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n        existing_scripts.push(script.src)\n      }\n    }\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (let i = 0; i < js_modules.length; i++) {\n      const url = js_modules[i];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      const url = js_exports[name];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) >= 0 || root[name] != null) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.holoviz.org/panel/1.8.2/dist/bundled/reactiveesm/es-module-shims@^1.10.0/dist/es-module-shims.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-3.8.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.8.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.8.0.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.8.0.min.js\", \"https://cdn.holoviz.org/panel/1.8.2/dist/panel.min.js\"];\n  const js_modules = [];\n  const js_exports = {};\n  const css_urls = [];\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (let i = 0; i < inline_js.length; i++) {\n        try {\n          inline_js[i].call(root, root.Bokeh);\n        } catch(e) {\n          if (!reloading) {\n            throw e;\n          }\n        }\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n        var NewBokeh = root.Bokeh;\n        if (Bokeh.versions === undefined) {\n          Bokeh.versions = new Map();\n        }\n        if (NewBokeh.version !== Bokeh.version) {\n          Bokeh.versions.set(NewBokeh.version, NewBokeh)\n        }\n        root.Bokeh = Bokeh;\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      // If the timeout and bokeh was not successfully loaded we reset\n      // everything and try loading again\n      root._bokeh_timeout = Date.now() + 5000;\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      root._bokeh_is_loading = 0\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      const bokeh_loaded = root.Bokeh != null && (root.Bokeh.version === py_version || (root.Bokeh.versions !== undefined && root.Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n        if (root.Bokeh) {\n          root.Bokeh = undefined;\n        }\n        console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n        console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n        run_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        let retries = 0;\n        const open = () => {\n          if (comm.active) {\n            comm.open();\n          } else if (retries > 3) {\n            console.warn('Comm target never activated')\n          } else {\n            retries += 1\n            setTimeout(open, 500)\n          }\n        }\n        if (comm.active) {\n          comm.open();\n        } else {\n          setTimeout(open, 500)\n        }\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        })\n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='9111c56b-9227-41db-a9f1-3b8b616ebb8d'>\n",
       "  <div id=\"b32d8ff3-ff0b-4a0d-afbc-458770054001\" data-root-id=\"9111c56b-9227-41db-a9f1-3b8b616ebb8d\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"22efb74e-02b7-476e-bc45-7d6e6865ebc4\":{\"version\":\"3.8.0\",\"title\":\"Bokeh Application\",\"config\":{\"type\":\"object\",\"name\":\"DocumentConfig\",\"id\":\"81d9ba9b-7f8c-4dd5-9a5f-f07d18ed3397\",\"attributes\":{\"notifications\":{\"type\":\"object\",\"name\":\"Notifications\",\"id\":\"c04459ef-6f3d-4ee9-9a1d-959349318ab5\"}}},\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"9111c56b-9227-41db-a9f1-3b8b616ebb8d\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"72a3dbed-dcb2-41d7-8b04-a8d1c4b8319a\",\"attributes\":{\"plot_id\":\"9111c56b-9227-41db-a9f1-3b8b616ebb8d\",\"comm_id\":\"58b29b12b22d4e5ea00a3b22c7538fbc\",\"client_comm_id\":\"6b7c4850e9144b0ca0c915140636f19c\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"start\",\"kind\":\"Any\",\"default\":0},{\"name\":\"end\",\"kind\":\"Any\",\"default\":100},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"ReactiveESM1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"JSComponent1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"ReactComponent1\",\"properties\":[{\"name\":\"use_shadow_dom\",\"kind\":\"Any\",\"default\":true},{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"AnyWidgetComponent1\",\"properties\":[{\"name\":\"use_shadow_dom\",\"kind\":\"Any\",\"default\":true},{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"max_notifications\",\"kind\":\"Any\",\"default\":5},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_rendered\",\"kind\":\"Any\",\"default\":false},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"request_value1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"_synced\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_request_sync\",\"kind\":\"Any\",\"default\":0}]}]}};\n",
       "  var render_items = [{\"docid\":\"22efb74e-02b7-476e-bc45-7d6e6865ebc4\",\"roots\":{\"9111c56b-9227-41db-a9f1-3b8b616ebb8d\":\"b32d8ff3-ff0b-4a0d-afbc-458770054001\"},\"root_ids\":[\"9111c56b-9227-41db-a9f1-3b8b616ebb8d\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  async function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t  for (const child of root_el.children) {\n",
       "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
       "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
       "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
       "\t  }\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined)\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "9111c56b-9227-41db-a9f1-3b8b616ebb8d"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05a1a54fd9764847a06cd75ea9e3b336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BokehModel(combine_events=True, render_bundle={'docs_json': {'3ca312c0-00ed-419e-b6cc-a4a9fd7bc630': {'version‚Ä¶"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import panel as pn  # GUI\n",
    "pn.extension()\n",
    "\n",
    "panels = [] # collect display \n",
    "\n",
    "context = [ {'role':'system', 'content':\"\"\"\n",
    "You are OrderBot, an automated service to collect orders for a pizza restaurant. \\\n",
    "You first greet the customer, then collects the order, \\\n",
    "and then asks if it's a pickup or delivery. \\\n",
    "You wait to collect the entire order, then summarize it and check for a final \\\n",
    "time if the customer wants to add anything else. \\\n",
    "If it's a delivery, you ask for an address. \\\n",
    "Finally you collect the payment.\\\n",
    "Make sure to clarify all options, extras and sizes to uniquely \\\n",
    "identify the item from the menu.\\\n",
    "You respond in a short, very conversational friendly style. \\\n",
    "The menu includes \\\n",
    "pepperoni pizza  12.95, 10.00, 7.00 \\\n",
    "cheese pizza   10.95, 9.25, 6.50 \\\n",
    "eggplant pizza   11.95, 9.75, 6.75 \\\n",
    "fries 4.50, 3.50 \\\n",
    "greek salad 7.25 \\\n",
    "Toppings: \\\n",
    "extra cheese 2.00, \\\n",
    "mushrooms 1.50 \\\n",
    "sausage 3.00 \\\n",
    "canadian bacon 3.50 \\\n",
    "AI sauce 1.50 \\\n",
    "peppers 1.00 \\\n",
    "Drinks: \\\n",
    "coke 3.00, 2.00, 1.00 \\\n",
    "sprite 3.00, 2.00, 1.00 \\\n",
    "bottled water 5.00 \\\n",
    "\"\"\"} ]  # accumulate messages\n",
    "\n",
    "\n",
    "inp = pn.widgets.TextInput(value=\"Hi\", placeholder='Enter text here‚Ä¶')\n",
    "button_conversation = pn.widgets.Button(name=\"Chat!\")\n",
    "\n",
    "interactive_conversation = pn.bind(collect_messages, button_conversation)\n",
    "\n",
    "dashboard = pn.Column(\n",
    "    inp,\n",
    "    pn.Row(button_conversation),\n",
    "    pn.panel(interactive_conversation, loading_indicator=True, height=300),\n",
    ")\n",
    "\n",
    "dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f298255",
   "metadata": {},
   "source": [
    "Now we go ahead and collect the user's order and process it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a29ae062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLMResponse(content=\"You've got a few options for where to get your Medium Cheese Pizza:\\n\\nWould you like to pick it up from our store, or have it delivered to your doorstep?\\n\\nIf you choose delivery, I'll need an address to make sure we bring it right to you. Just let me know!\", provider=<LLMProvider.OLLAMA: 'ollama'>, model='llama3:8b', usage={'prompt_tokens': 675, 'completion_tokens': 60, 'total_tokens': 735}, latency=26.801153898239136, local=False)\n"
     ]
    }
   ],
   "source": [
    "messages =  context.copy()\n",
    "messages.append(\n",
    "{'role':'system', 'content':'create a json summary of the previous food order. Itemize the price for each item\\\n",
    " The fields should be 1) pizza, include size 2) list of toppings 3) list of drinks, include size   4) list of sides include size  5)total price '},    \n",
    ")\n",
    " #The fields should be 1) pizza, price 2) list of toppings 3) list of drinks, include size include price  4) list of sides include size include price, 5)total price '},    \n",
    "\n",
    "response = llm_client.chat_completion(LLMProvider.OLLAMA, messages=messages, temperature=0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb13505a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
