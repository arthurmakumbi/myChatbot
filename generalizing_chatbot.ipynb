{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22084a51",
   "metadata": {},
   "source": [
    "# My LLM Client Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244e5da1",
   "metadata": {},
   "source": [
    "On my journey of developing smart chatbots, i ran into rate limits while experimenting with open ai. This was after I had spent hours setting up my system. I knew that was not the end of the world. I then decided to implement a generalized way to program llms. so here we go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b17e516",
   "metadata": {},
   "source": [
    "## Overview\n",
    "In this notebook, I implement a multi-provider LLM client with error handling and retry logic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e096bbb",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Firstly, I install dependencies and set up API keys...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ea449c",
   "metadata": {},
   "source": [
    "### install dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75374c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this cell once, then comment it out)\n",
    "# %pip install openai\n",
    "\n",
    "# %pip install openai # installs the OpenAI API client.\n",
    "# %pip install python-dotenv # installs the dotenv package for loading environment variables.\n",
    "\n",
    "# %pip install openai google-generativeai anthropic tenacity python-dotenv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23240a98",
   "metadata": {},
   "source": [
    "### import libraries and set up api keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e6d7903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything needed\n",
    "import os\n",
    "from enum import Enum\n",
    "from typing import List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import openai\n",
    "from openai import OpenAI as OpenAIClient\n",
    "import google.generativeai as genai\n",
    "import anthropic\n",
    "from tenacity import retry, wait_exponential, stop_after_attempt, retry_if_exception_type\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Set API keys (or load from .env file)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ['DEEPSEEK_API_KEY'] = os.getenv('DEEPSEEK_API_KEY')  \n",
    "os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "# ... etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fce7f11",
   "metadata": {},
   "source": [
    "### data classes and enums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9591aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining data structures i will use\n",
    "class LLMProvider(Enum):\n",
    "    OPENAI = \"openai\"\n",
    "    DEEPSEEK = \"deepseek\"\n",
    "    GOOGLE = \"google\"\n",
    "    ANTHROPIC = \"anthropic\"\n",
    "\n",
    "@dataclass\n",
    "class LLMResponse:\n",
    "    content: str\n",
    "    provider: LLMProvider\n",
    "    model: str\n",
    "    usage: Optional[Dict] = None\n",
    "    latency: Optional[float] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b8c5d9",
   "metadata": {},
   "source": [
    "### main LLM client class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "edf617c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProfessionalLLMClient:\n",
    "    \"\"\"Main client class to interact with various LLM providers.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.clients = {}\n",
    "        self.available_providers = []\n",
    "        self._initialize_all_clients()\n",
    "\n",
    "    \n",
    "    def _initialize_all_clients(self):\n",
    "        \"\"\"Initialize all available API clients.\"\"\"\n",
    "        provider_configs = {\n",
    "            LLMProvider.OPENAI: {\n",
    "                'env_var': 'OPENAI_API_KEY',\n",
    "                'init_func': self._init_openai\n",
    "            },\n",
    "            LLMProvider.DEEPSEEK: {\n",
    "                'env_var': 'DEEPSEEK_API_KEY', \n",
    "                'init_func': self._init_deepseek\n",
    "            },\n",
    "            LLMProvider.GOOGLE: {\n",
    "                'env_var': 'GOOGLE_API_KEY',\n",
    "                'init_func': self._init_google\n",
    "            },\n",
    "            LLMProvider.ANTHROPIC: {\n",
    "                'env_var': 'ANTHROPIC_API_KEY',\n",
    "                'init_func': self._init_anthropic\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for provider, config in provider_configs.items():\n",
    "            api_key = os.getenv(config['env_var'])\n",
    "            if api_key and not api_key.startswith('your-'):\n",
    "                try:\n",
    "                    config['init_func'](api_key)\n",
    "                    self.available_providers.append(provider)\n",
    "                    print(f\"âœ… {provider.value.upper()} client initialized successfully\")\n",
    "                except Exception as e:\n",
    "                    print(f\"âŒ Failed to initialize {provider.value}: {e}\")\n",
    "            else:\n",
    "                print(f\"âš ï¸  Skipping {provider.value}: API key not found\")\n",
    "    \n",
    "    def _init_openai(self, api_key: str):\n",
    "        \"\"\"Initialize OpenAI client.\"\"\"\n",
    "        self.clients[LLMProvider.OPENAI] = OpenAIClient(api_key=api_key)\n",
    "    \n",
    "    def _init_deepseek(self, api_key: str):\n",
    "        \"\"\"Initialize DeepSeek client.\"\"\"\n",
    "        self.clients[LLMProvider.DEEPSEEK] = OpenAIClient(\n",
    "            api_key=api_key,\n",
    "            base_url=\"https://api.deepseek.com/v1\"\n",
    "        )\n",
    "    \n",
    "    def _init_google(self, api_key: str):\n",
    "        \"\"\"Initialize Google client.\"\"\"\n",
    "        genai.configure(api_key=api_key)\n",
    "        self.clients[LLMProvider.GOOGLE] = genai\n",
    "    \n",
    "    def _init_anthropic(self, api_key: str):\n",
    "        \"\"\"Initialize Anthropic client.\"\"\"\n",
    "        self.clients[LLMProvider.ANTHROPIC] = anthropic.Anthropic(api_key=api_key)\n",
    "    \n",
    "    @retry(\n",
    "        retry=retry_if_exception_type((\n",
    "            openai.RateLimitError,\n",
    "            anthropic.RateLimitError,\n",
    "            openai.APIConnectionError,\n",
    "            openai.APIError\n",
    "        )),\n",
    "        wait=wait_exponential(multiplier=1, min=2, max=30),\n",
    "        stop=stop_after_attempt(3),\n",
    "        reraise=True\n",
    "    )\n",
    "    def chat_completion(\n",
    "        self,\n",
    "        provider: LLMProvider,\n",
    "        messages: List[Dict[str, str]],\n",
    "        model: Optional[str] = None,\n",
    "        temperature: float = 0.7,\n",
    "        max_tokens: Optional[int] = None,\n",
    "        **kwargs\n",
    "    ) -> LLMResponse:\n",
    "        \"\"\"\n",
    "        Chat completion with comprehensive error handling.\n",
    "        \n",
    "        Args:\n",
    "            provider: The LLM provider to use\n",
    "            messages: List of message dictionaries\n",
    "            model: Specific model name (uses default if None)\n",
    "            temperature: Creativity control (0.0 to 1.0)\n",
    "            max_tokens: Maximum tokens to generate\n",
    "            \n",
    "        Returns:\n",
    "            LLMResponse object with structured data\n",
    "        \"\"\"\n",
    "        \n",
    "        if provider not in self.clients:\n",
    "            raise ValueError(f\"Provider {provider.value} not available\")\n",
    "        \n",
    "        # Set default models\n",
    "        default_models = {\n",
    "            LLMProvider.OPENAI: \"gpt-3.5-turbo\",\n",
    "            LLMProvider.DEEPSEEK: \"deepseek-chat\",\n",
    "            LLMProvider.GOOGLE: \"gemini-1.0-pro\",  # Updated from gemini-pro\n",
    "            LLMProvider.ANTHROPIC: \"claude-3-haiku-20240307\"\n",
    "        }\n",
    "        model = model or default_models[provider]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            if provider in [LLMProvider.OPENAI, LLMProvider.DEEPSEEK]:\n",
    "                response = self.clients[provider].chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=messages,\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=max_tokens,\n",
    "                    **kwargs\n",
    "                )\n",
    "                content = response.choices[0].message.content\n",
    "                usage = {\n",
    "                    'prompt_tokens': response.usage.prompt_tokens,\n",
    "                    'completion_tokens': response.usage.completion_tokens,\n",
    "                    'total_tokens': response.usage.total_tokens\n",
    "                } if hasattr(response, 'usage') and response.usage else None\n",
    "                \n",
    "            elif provider == LLMProvider.GOOGLE:\n",
    "                # UPDATED: Google Gemini API format\n",
    "                conversation = []\n",
    "                for msg in messages:\n",
    "                    if msg[\"role\"] == \"system\":\n",
    "                        # For system messages, prepend to first user message\n",
    "                        if conversation and conversation[-1][\"role\"] == \"user\":\n",
    "                            conversation[-1][\"parts\"][0] = f\"{msg['content']}\\n\\n{conversation[-1]['parts'][0]}\"\n",
    "                        else:\n",
    "                            # If no user message yet, store system for later\n",
    "                            system_content = msg[\"content\"]\n",
    "                    else:\n",
    "                        role = \"user\" if msg[\"role\"] == \"user\" else \"model\"\n",
    "                        conversation.append({\"role\": role, \"parts\": [msg[\"content\"]]})\n",
    "                \n",
    "                # Create the model\n",
    "                model_obj = self.clients[provider].GenerativeModel(\n",
    "                    model_name=model,\n",
    "                    generation_config={\n",
    "                        \"temperature\": temperature,\n",
    "                        \"max_output_tokens\": max_tokens or 2048,\n",
    "                        \"top_p\": kwargs.get(\"top_p\", 0.95),\n",
    "                        \"top_k\": kwargs.get(\"top_k\", 40),\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Start chat and send message\n",
    "                chat = model_obj.start_chat(history=conversation[:-1] if len(conversation) > 1 else [])\n",
    "                response = chat.send_message(conversation[-1][\"parts\"][0] if conversation else \"\")\n",
    "                \n",
    "                content = response.text\n",
    "                usage = {\n",
    "                    'prompt_tokens': response.usage_metadata.prompt_token_count if hasattr(response, 'usage_metadata') else None,\n",
    "                    'completion_tokens': response.usage_metadata.candidates_token_count if hasattr(response, 'usage_metadata') else None,\n",
    "                    'total_tokens': response.usage_metadata.total_token_count if hasattr(response, 'usage_metadata') else None\n",
    "                }\n",
    "\n",
    "                \n",
    "            elif provider == LLMProvider.ANTHROPIC:\n",
    "                # Convert to Anthropic's format\n",
    "                system_content = None\n",
    "                anthropic_messages = []\n",
    "                \n",
    "                for msg in messages:\n",
    "                    if msg[\"role\"] == \"system\":\n",
    "                        system_content = msg[\"content\"]\n",
    "                    else:\n",
    "                        role = \"user\" if msg[\"role\"] == \"user\" else \"assistant\"\n",
    "                        anthropic_messages.append({\"role\": role, \"content\": msg[\"content\"]})\n",
    "                \n",
    "                response = self.clients[provider].messages.create(\n",
    "                    model=model,\n",
    "                    messages=anthropic_messages,\n",
    "                    system=system_content,\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=max_tokens or 1024,\n",
    "                    **kwargs\n",
    "                )\n",
    "                content = response.content[0].text\n",
    "                usage = {\n",
    "                    'input_tokens': response.usage.input_tokens,\n",
    "                    'output_tokens': response.usage.output_tokens\n",
    "                }\n",
    "                \n",
    "            latency = time.time() - start_time\n",
    "            \n",
    "            return LLMResponse(\n",
    "                content=content,\n",
    "                provider=provider,\n",
    "                model=model,\n",
    "                usage=usage,\n",
    "                latency=latency\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error with {provider.value} API: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def get_available_providers(self) -> List[LLMProvider]:\n",
    "        \"\"\"Get list of successfully initialized providers.\"\"\"\n",
    "        return self.available_providers\n",
    "    \n",
    "    def get_provider_models(self, provider: LLMProvider) -> List[str]:\n",
    "        \"\"\"Get available models for a provider.\"\"\"\n",
    "        model_lists = {\n",
    "            LLMProvider.OPENAI: [\"gpt-4-turbo\", \"gpt-4\", \"gpt-3.5-turbo\", \"gpt-3.5-turbo-16k\"],\n",
    "            LLMProvider.DEEPSEEK: [\"deepseek-chat\", \"deepseek-coder\"],\n",
    "            LLMProvider.GOOGLE: [\n",
    "                \"gemini-1.0-pro\",      # General purpose\n",
    "                \"gemini-1.0-pro-001\",  # Specific version\n",
    "                \"gemini-pro\",           # Some accounts might still have this\n",
    "                \"gemini-1.5-pro-latest\" # Latest 1.5 pro (if you have access)\n",
    "            ],\n",
    "            LLMProvider.ANTHROPIC: [\n",
    "                \"claude-3-opus-20240229\", \n",
    "                \"claude-3-sonnet-20240229\", \n",
    "                \"claude-3-haiku-20240307\"\n",
    "            ]\n",
    "        }\n",
    "        return model_lists.get(provider, [])\n",
    "    \n",
    "    def batch_process(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        provider: LLMProvider,\n",
    "        **kwargs\n",
    "    ) -> List[LLMResponse]:\n",
    "        \"\"\"Process multiple prompts with the same settings.\"\"\"\n",
    "        results = []\n",
    "        for prompt in prompts:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            results.append(self.chat_completion(provider, messages, **kwargs))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d64d76",
   "metadata": {},
   "source": [
    "### initializing my LLM client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c56777d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… OPENAI client initialized successfully\n",
      "âœ… DEEPSEEK client initialized successfully\n",
      "âœ… GOOGLE client initialized successfully\n",
      "âš ï¸  Skipping anthropic: API key not found\n",
      "Available providers: ['openai', 'deepseek', 'google']\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of my llm client\n",
    "llm_client = ProfessionalLLMClient()\n",
    "\n",
    "# Check what providers are available\n",
    "print(\"Available providers:\", [p.value for p in llm_client.get_available_providers()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71c63af",
   "metadata": {},
   "source": [
    "## Usage Examples\n",
    "Here's how to use the client for different providers..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535928a7",
   "metadata": {},
   "source": [
    "### testing my individual providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57495be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with deepseek API: Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Error with deepseek API: Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Error with deepseek API: Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Error: Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}\n"
     ]
    }
   ],
   "source": [
    "# Testing my LLM providers, one at a time\n",
    "# starting with DeepSeek\n",
    "test_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Explain quantum computing simply\"}\n",
    "]\n",
    "\n",
    "try:\n",
    "    response = llm_client.chat_completion(LLMProvider.DEEPSEEK, test_messages)\n",
    "    print(\"Google Response:\", response.content)\n",
    "    print(f\"Latency: {response.latency:.2f}s\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2049c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Google models: ['gemini-1.0-pro', 'gemini-1.0-pro-001', 'gemini-pro', 'gemini-1.5-pro-latest']\n",
      "\n",
      "ðŸ§ª Testing Google model: gemini-1.0-pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1758062125.575917 2046204 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with google API: 404 models/gemini-1.0-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "âŒ gemini-1.0-pro failed: 404 models/gemini-1.0-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "\n",
      "ðŸ§ª Testing Google model: gemini-1.0-pro-001\n",
      "Error with google API: 404 models/gemini-1.0-pro-001 is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "âŒ gemini-1.0-pro-001 failed: 404 models/gemini-1.0-pro-001 is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "\n",
      "ðŸ§ª Testing Google model: gemini-pro\n",
      "Error with google API: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "âŒ gemini-pro failed: 404 models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "\n",
      "ðŸ§ª Testing Google model: gemini-1.5-pro-latest\n",
      "Error with google API: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 33\n",
      "}\n",
      "]\n",
      "âŒ gemini-1.5-pro-latest failed: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_input_token_count\"\n",
      "  quota_id: \"GenerateContentInputTokensPerModelPerMinute-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerMinutePerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      "violations {\n",
      "  quota_metric: \"generativelanguage.googleapis.com/generate_content_free_tier_requests\"\n",
      "  quota_id: \"GenerateRequestsPerDayPerProjectPerModel-FreeTier\"\n",
      "  quota_dimensions {\n",
      "    key: \"model\"\n",
      "    value: \"gemini-1.5-pro\"\n",
      "  }\n",
      "  quota_dimensions {\n",
      "    key: \"location\"\n",
      "    value: \"global\"\n",
      "  }\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 33\n",
      "}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Test Google Gemini specifically with different models\n",
    "test_messages = [{\"role\": \"user\", \"content\": \"Hello! What's the weather like today?\"}]\n",
    "\n",
    "# Try available Google models\n",
    "google_models = llm_client.get_provider_models(LLMProvider.GOOGLE)\n",
    "print(\"Available Google models:\", google_models)\n",
    "\n",
    "for model in google_models:\n",
    "    try:\n",
    "        print(f\"\\nðŸ§ª Testing Google model: {model}\")\n",
    "        response = llm_client.chat_completion(\n",
    "            LLMProvider.GOOGLE, \n",
    "            test_messages, \n",
    "            model=model\n",
    "        )\n",
    "        print(f\"âœ… Success with {model}: {response.content[:100]}...\")\n",
    "        break  # Stop after first successful model\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {model} failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c87e5b6",
   "metadata": {},
   "source": [
    "## Comparison Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12b8e07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing openai...\n",
      "Error with openai API: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Error with openai API: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Error with openai API: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "âŒ openai failed: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "--------------------------------------------------\n",
      "ðŸ§ª Testing deepseek...\n",
      "Error with deepseek API: Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Error with deepseek API: Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "Error with deepseek API: Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "âŒ deepseek failed: Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "--------------------------------------------------\n",
      "ðŸ§ª Testing google...\n",
      "Error with google API: 404 models/gemini-1.0-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "âŒ google failed: 404 models/gemini-1.0-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Compare multiple providers\n",
    "def compare_providers_professional(prompt: str):\n",
    "    \"\"\"Professional comparison of all available providers.\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    results = {}\n",
    "    \n",
    "    for provider in llm_client.get_available_providers():\n",
    "        try:\n",
    "            print(f\"ðŸ§ª Testing {provider.value}...\")\n",
    "            response = llm_client.chat_completion(provider, messages, temperature=0.7)\n",
    "            results[provider.value] = {\n",
    "                'content': response.content,\n",
    "                'latency': response.latency,\n",
    "                'tokens': response.usage\n",
    "            }\n",
    "            print(f\"âœ… {provider.value} completed in {response.latency:.2f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            results[provider.value] = f\"Error: {str(e)}\"\n",
    "            print(f\"âŒ {provider.value} failed: {e}\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run comparison\n",
    "comparison_results = compare_providers_professional(\n",
    "    \"What are the main benefits of machine learning in healthcare?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01a673f",
   "metadata": {},
   "source": [
    "### Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59bc09f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âŒ OPENAI: Error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯\n",
      "\n",
      "âŒ DEEPSEEK: Error: Error code: 402 - {'error': {'message': 'Insufficient Balance', 'type': 'unknown_error', 'param': None, 'code': 'invalid_request_error'}}\n",
      "âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯\n",
      "\n",
      "âŒ GOOGLE: Error: 404 models/gemini-1.0-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
      "âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯âŽ¯\n"
     ]
    }
   ],
   "source": [
    "# Display results in a nice format\n",
    "for provider, data in comparison_results.items():\n",
    "    if isinstance(data, dict):\n",
    "        print(f\"\\nðŸŽ¯ {provider.upper()}:\")\n",
    "        print(f\"â±ï¸  Latency: {data['latency']:.2f}s\")\n",
    "        print(f\"ðŸ“Š Tokens: {data['tokens']}\")\n",
    "        print(f\"ðŸ’¬ Response: {data['content'][:200]}...\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ {provider.upper()}: {data}\")\n",
    "    print(\"âŽ¯\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731c1fe1",
   "metadata": {},
   "source": [
    "## Experimenting with Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c9bd3b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch of prompts with DeepSeek...\n",
      "Error with openai API: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Error with openai API: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Error with openai API: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      2\u001b[39m prompts = [\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mExplain blockchain technology\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWhat is the capital of France?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mHow does photosynthesis work?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m ]\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mProcessing batch of prompts with DeepSeek...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m batch_results = llm_client.batch_process(\n\u001b[32m     10\u001b[39m     prompts=prompts,\n\u001b[32m     11\u001b[39m     provider=LLMProvider.OPENAI,\n\u001b[32m     12\u001b[39m     temperature=\u001b[32m0.3\u001b[39m\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, result \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_results):\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mðŸ“ Prompt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompts[i][:\u001b[32m30\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 238\u001b[39m, in \u001b[36mProfessionalLLMClient.batch_process\u001b[39m\u001b[34m(self, prompts, provider, **kwargs)\u001b[39m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[32m    237\u001b[39m     messages = [{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: prompt}]\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m     results.append(\u001b[38;5;28mself\u001b[39m.chat_completion(provider, messages, **kwargs))\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jupyter_env/lib/python3.13/site-packages/tenacity/__init__.py:338\u001b[39m, in \u001b[36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    336\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    337\u001b[39m wrapped_f.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m copy(f, *args, **kw)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jupyter_env/lib/python3.13/site-packages/tenacity/__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28mself\u001b[39m.iter(retry_state=retry_state)\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jupyter_env/lib/python3.13/site-packages/tenacity/__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = action(retry_state)\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jupyter_env/lib/python3.13/site-packages/tenacity/__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    418\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m retry_exc.reraise()\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jupyter_env/lib/python3.13/site-packages/tenacity/__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.result()\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jupyter_env/lib/python3.13/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_result()\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jupyter_env/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jupyter_env/lib/python3.13/site-packages/tenacity/__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = fn(*args, **kwargs)\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 113\u001b[39m, in \u001b[36mProfessionalLLMClient.chat_completion\u001b[39m\u001b[34m(self, provider, messages, model, temperature, max_tokens, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m provider \u001b[38;5;129;01min\u001b[39;00m [LLMProvider.OPENAI, LLMProvider.DEEPSEEK]:\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m         response = \u001b[38;5;28mself\u001b[39m.clients[provider].chat.completions.create(\n\u001b[32m    114\u001b[39m             model=model,\n\u001b[32m    115\u001b[39m             messages=messages,\n\u001b[32m    116\u001b[39m             temperature=temperature,\n\u001b[32m    117\u001b[39m             max_tokens=max_tokens,\n\u001b[32m    118\u001b[39m             **kwargs\n\u001b[32m    119\u001b[39m         )\n\u001b[32m    120\u001b[39m         content = response.choices[\u001b[32m0\u001b[39m].message.content\n\u001b[32m    121\u001b[39m         usage = {\n\u001b[32m    122\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mprompt_tokens\u001b[39m\u001b[33m'\u001b[39m: response.usage.prompt_tokens,\n\u001b[32m    123\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mcompletion_tokens\u001b[39m\u001b[33m'\u001b[39m: response.usage.completion_tokens,\n\u001b[32m    124\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mtotal_tokens\u001b[39m\u001b[33m'\u001b[39m: response.usage.total_tokens\n\u001b[32m    125\u001b[39m         } \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(response, \u001b[33m'\u001b[39m\u001b[33musage\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m response.usage \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jupyter_env/lib/python3.13/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jupyter_env/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:1147\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1144\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1145\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1146\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   1148\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1149\u001b[39m         body=maybe_transform(\n\u001b[32m   1150\u001b[39m             {\n\u001b[32m   1151\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   1152\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   1153\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   1154\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   1155\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   1156\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   1157\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   1158\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   1159\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   1160\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   1161\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   1162\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   1163\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   1164\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   1165\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   1166\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   1167\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   1168\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   1169\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   1170\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   1171\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   1172\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   1173\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   1174\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   1175\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   1176\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   1177\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   1178\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   1179\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   1180\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   1181\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   1182\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   1183\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: verbosity,\n\u001b[32m   1184\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   1185\u001b[39m             },\n\u001b[32m   1186\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   1187\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   1188\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   1189\u001b[39m         ),\n\u001b[32m   1190\u001b[39m         options=make_request_options(\n\u001b[32m   1191\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   1192\u001b[39m         ),\n\u001b[32m   1193\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   1194\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1195\u001b[39m         stream_cls=Stream[ChatCompletionChunk],\n\u001b[32m   1196\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jupyter_env/lib/python3.13/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/jupyter_env/lib/python3.13/site-packages/openai/_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "# Process multiple prompts at once\n",
    "prompts = [\n",
    "    \"Explain blockchain technology\",\n",
    "    \"What is the capital of France?\",\n",
    "    \"How does photosynthesis work?\"\n",
    "]\n",
    "\n",
    "print(\"Processing batch of prompts with DeepSeek...\")\n",
    "batch_results = llm_client.batch_process(\n",
    "    prompts=prompts,\n",
    "    provider=LLMProvider.OPENAI,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "for i, result in enumerate(batch_results):\n",
    "    print(f\"\\nðŸ“ Prompt {i+1}: {prompts[i][:30]}...\")\n",
    "    print(f\"ðŸ’¡ Response: {result.content[:100]}...\")\n",
    "    print(f\"â±ï¸  Latency: {result.latency:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea32a0ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
